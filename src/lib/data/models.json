[
  {
    "id": "llama-3.2-1b-q8",
    "name": "Llama 3.2 1B",
    "params_b": 1.24,
    "quantization": "Q8_0",
    "weight_gb": 1.32,
    "kv_per_1k_gb": 0.031,
    "max_context_k": 128,
    "mmlu_score": 49.3,
    "swe_bench_score": null,
    "features": ["tool_use"],
    "notes": "16 layers, 8 KV heads, head_dim 64. GGUF Q8_0 file size."
  },
  {
    "id": "llama-3.2-1b-q4",
    "name": "Llama 3.2 1B",
    "params_b": 1.24,
    "quantization": "Q4_K_M",
    "weight_gb": 0.81,
    "kv_per_1k_gb": 0.031,
    "max_context_k": 128,
    "mmlu_score": 49.3,
    "swe_bench_score": null,
    "features": ["tool_use"],
    "notes": "16 layers, 8 KV heads, head_dim 64. GGUF Q4_K_M file size."
  },

  {
    "id": "llama-3.2-3b-fp16",
    "name": "Llama 3.2 3B",
    "params_b": 3.21,
    "quantization": "fp16",
    "weight_gb": 6.43,
    "kv_per_1k_gb": 0.107,
    "max_context_k": 128,
    "mmlu_score": 63.4,
    "swe_bench_score": null,
    "features": ["tool_use"],
    "notes": "28 layers, 8 KV heads, head_dim 128. GGUF fp16 file size."
  },
  {
    "id": "llama-3.2-3b-q8",
    "name": "Llama 3.2 3B",
    "params_b": 3.21,
    "quantization": "Q8_0",
    "weight_gb": 3.42,
    "kv_per_1k_gb": 0.107,
    "max_context_k": 128,
    "mmlu_score": 63.4,
    "swe_bench_score": null,
    "features": ["tool_use"],
    "notes": "28 layers, 8 KV heads, head_dim 128. GGUF Q8_0 file size."
  },
  {
    "id": "llama-3.2-3b-q4",
    "name": "Llama 3.2 3B",
    "params_b": 3.21,
    "quantization": "Q4_K_M",
    "weight_gb": 2.02,
    "kv_per_1k_gb": 0.107,
    "max_context_k": 128,
    "mmlu_score": 63.4,
    "swe_bench_score": null,
    "features": ["tool_use"],
    "notes": "28 layers, 8 KV heads, head_dim 128. GGUF Q4_K_M file size."
  },

  {
    "id": "llama-3.1-8b-fp16",
    "name": "Llama 3.1 8B",
    "params_b": 8.03,
    "quantization": "fp16",
    "weight_gb": 16.05,
    "kv_per_1k_gb": 0.122,
    "max_context_k": 128,
    "mmlu_score": 69.4,
    "swe_bench_score": 6.2,
    "features": ["tool_use"],
    "notes": "32 layers, 8 KV heads, head_dim 128. Estimated from GGUF f32 file size (32.1 GB) / 2."
  },
  {
    "id": "llama-3.1-8b-q8",
    "name": "Llama 3.1 8B",
    "params_b": 8.03,
    "quantization": "Q8_0",
    "weight_gb": 8.54,
    "kv_per_1k_gb": 0.122,
    "max_context_k": 128,
    "mmlu_score": 69.4,
    "swe_bench_score": 6.2,
    "features": ["tool_use"],
    "notes": "32 layers, 8 KV heads, head_dim 128. GGUF Q8_0 file size."
  },
  {
    "id": "llama-3.1-8b-q4",
    "name": "Llama 3.1 8B",
    "params_b": 8.03,
    "quantization": "Q4_K_M",
    "weight_gb": 4.92,
    "kv_per_1k_gb": 0.122,
    "max_context_k": 128,
    "mmlu_score": 69.4,
    "swe_bench_score": 6.2,
    "features": ["tool_use"],
    "notes": "32 layers, 8 KV heads, head_dim 128. GGUF Q4_K_M file size."
  },

  {
    "id": "llama-3.2-11b-vision-q8",
    "name": "Llama 3.2 Vision 11B",
    "params_b": 10.6,
    "quantization": "Q8_0",
    "weight_gb": 10.4,
    "kv_per_1k_gb": 0.122,
    "max_context_k": 128,
    "mmlu_score": 73.0,
    "swe_bench_score": null,
    "features": ["vision", "tool_use"],
    "notes": "Multimodal (vision adapter on Llama 3.1 8B). 40 layers (32 self-attn + 8 cross-attn), 8 KV heads, head_dim 128. GGUF Q8_0 file size."
  },
  {
    "id": "llama-3.2-11b-vision-q4",
    "name": "Llama 3.2 Vision 11B",
    "params_b": 10.6,
    "quantization": "Q4_K_M",
    "weight_gb": 5.96,
    "kv_per_1k_gb": 0.122,
    "max_context_k": 128,
    "mmlu_score": 73.0,
    "swe_bench_score": null,
    "features": ["vision", "tool_use"],
    "notes": "Multimodal (vision adapter on Llama 3.1 8B). 40 layers (32 self-attn + 8 cross-attn), 8 KV heads, head_dim 128. GGUF Q4_K_M file size."
  },

  {
    "id": "llama-3.3-70b-q8",
    "name": "Llama 3.3 70B",
    "params_b": 70.6,
    "quantization": "Q8_0",
    "weight_gb": 74.98,
    "kv_per_1k_gb": 0.305,
    "max_context_k": 128,
    "mmlu_score": 86.0,
    "swe_bench_score": 24.8,
    "features": ["tool_use"],
    "notes": "80 layers, 8 KV heads, head_dim 128. GGUF Q8_0 file size."
  },
  {
    "id": "llama-3.3-70b-q4",
    "name": "Llama 3.3 70B",
    "params_b": 70.6,
    "quantization": "Q4_K_M",
    "weight_gb": 42.52,
    "kv_per_1k_gb": 0.305,
    "max_context_k": 128,
    "mmlu_score": 86.0,
    "swe_bench_score": 24.8,
    "features": ["tool_use"],
    "notes": "80 layers, 8 KV heads, head_dim 128. GGUF Q4_K_M file size."
  },

  {
    "id": "llama-4-scout-q8",
    "name": "Llama 4 Scout",
    "params_b": 109,
    "quantization": "Q8_0",
    "weight_gb": 115,
    "kv_per_1k_gb": 0.183,
    "max_context_k": 128,
    "mmlu_score": 79.6,
    "swe_bench_score": null,
    "features": ["vision", "tool_use"],
    "notes": "MoE: 109B total, 17B active (16 experts). 48 layers, 8 KV heads, head_dim 128, hybrid attention (12 global + 36 local, chunk=8192). GGUF Q8_0 file size."
  },
  {
    "id": "llama-4-scout-q4",
    "name": "Llama 4 Scout",
    "params_b": 109,
    "quantization": "Q4_K_M",
    "weight_gb": 65.4,
    "kv_per_1k_gb": 0.183,
    "max_context_k": 128,
    "mmlu_score": 79.6,
    "swe_bench_score": null,
    "features": ["vision", "tool_use"],
    "notes": "MoE: 109B total, 17B active (16 experts). 48 layers, 8 KV heads, head_dim 128, hybrid attention (12 global + 36 local, chunk=8192). GGUF Q4_K_M file size."
  },

  {
    "id": "llama-4-maverick-q8",
    "name": "Llama 4 Maverick",
    "params_b": 400,
    "quantization": "Q8_0",
    "weight_gb": 426,
    "kv_per_1k_gb": 0.183,
    "max_context_k": 128,
    "mmlu_score": 85.5,
    "swe_bench_score": null,
    "features": ["vision", "tool_use"],
    "notes": "MoE: 400B total, 17B active (128 experts). 48 layers, 8 KV heads, head_dim 128. GGUF Q8_0 file size."
  },
  {
    "id": "llama-4-maverick-q4",
    "name": "Llama 4 Maverick",
    "params_b": 400,
    "quantization": "Q4_K_M",
    "weight_gb": 243,
    "kv_per_1k_gb": 0.183,
    "max_context_k": 128,
    "mmlu_score": 85.5,
    "swe_bench_score": null,
    "features": ["vision", "tool_use"],
    "notes": "MoE: 400B total, 17B active (128 experts). 48 layers, 8 KV heads, head_dim 128. GGUF Q4_K_M file size."
  },

  {
    "id": "qwen-2.5-3b-q8",
    "name": "Qwen 2.5 3B",
    "params_b": 3.09,
    "quantization": "Q8_0",
    "weight_gb": 3.29,
    "kv_per_1k_gb": 0.034,
    "max_context_k": 32,
    "mmlu_score": 65.6,
    "swe_bench_score": null,
    "features": ["tool_use"],
    "notes": "36 layers, 2 KV heads, head_dim 128. GGUF Q8_0 file size."
  },
  {
    "id": "qwen-2.5-3b-q4",
    "name": "Qwen 2.5 3B",
    "params_b": 3.09,
    "quantization": "Q4_K_M",
    "weight_gb": 1.93,
    "kv_per_1k_gb": 0.034,
    "max_context_k": 32,
    "mmlu_score": 65.6,
    "swe_bench_score": null,
    "features": ["tool_use"],
    "notes": "36 layers, 2 KV heads, head_dim 128. GGUF Q4_K_M file size."
  },

  {
    "id": "qwen-2.5-7b-q8",
    "name": "Qwen 2.5 7B",
    "params_b": 7.62,
    "quantization": "Q8_0",
    "weight_gb": 8.10,
    "kv_per_1k_gb": 0.053,
    "max_context_k": 128,
    "mmlu_score": 74.2,
    "swe_bench_score": 11.6,
    "features": ["tool_use"],
    "notes": "28 layers, 4 KV heads, head_dim 128. GGUF Q8_0 file size."
  },
  {
    "id": "qwen-2.5-7b-q4",
    "name": "Qwen 2.5 7B",
    "params_b": 7.62,
    "quantization": "Q4_K_M",
    "weight_gb": 4.68,
    "kv_per_1k_gb": 0.053,
    "max_context_k": 128,
    "mmlu_score": 74.2,
    "swe_bench_score": 11.6,
    "features": ["tool_use"],
    "notes": "28 layers, 4 KV heads, head_dim 128. GGUF Q4_K_M file size."
  },

  {
    "id": "qwen-2.5-14b-q8",
    "name": "Qwen 2.5 14B",
    "params_b": 14.7,
    "quantization": "Q8_0",
    "weight_gb": 15.70,
    "kv_per_1k_gb": 0.183,
    "max_context_k": 128,
    "mmlu_score": 79.9,
    "swe_bench_score": 17.8,
    "features": ["tool_use"],
    "notes": "48 layers, 8 KV heads, head_dim 128. GGUF Q8_0 file size."
  },
  {
    "id": "qwen-2.5-14b-q4",
    "name": "Qwen 2.5 14B",
    "params_b": 14.7,
    "quantization": "Q4_K_M",
    "weight_gb": 8.99,
    "kv_per_1k_gb": 0.183,
    "max_context_k": 128,
    "mmlu_score": 79.9,
    "swe_bench_score": 17.8,
    "features": ["tool_use"],
    "notes": "48 layers, 8 KV heads, head_dim 128. GGUF Q4_K_M file size."
  },

  {
    "id": "qwen-2.5-32b-q8",
    "name": "Qwen 2.5 32B",
    "params_b": 32.5,
    "quantization": "Q8_0",
    "weight_gb": 34.82,
    "kv_per_1k_gb": 0.244,
    "max_context_k": 128,
    "mmlu_score": 83.3,
    "swe_bench_score": 24.2,
    "features": ["tool_use"],
    "notes": "64 layers, 8 KV heads, head_dim 128. GGUF Q8_0 file size."
  },
  {
    "id": "qwen-2.5-32b-q4",
    "name": "Qwen 2.5 32B",
    "params_b": 32.5,
    "quantization": "Q4_K_M",
    "weight_gb": 19.85,
    "kv_per_1k_gb": 0.244,
    "max_context_k": 128,
    "mmlu_score": 83.3,
    "swe_bench_score": 24.2,
    "features": ["tool_use"],
    "notes": "64 layers, 8 KV heads, head_dim 128. GGUF Q4_K_M file size."
  },

  {
    "id": "qwen-2.5-72b-q8",
    "name": "Qwen 2.5 72B",
    "params_b": 72.7,
    "quantization": "Q8_0",
    "weight_gb": 77.26,
    "kv_per_1k_gb": 0.305,
    "max_context_k": 128,
    "mmlu_score": 86.1,
    "swe_bench_score": 28.4,
    "features": ["tool_use"],
    "notes": "80 layers, 8 KV heads, head_dim 128. GGUF Q8_0 file size."
  },
  {
    "id": "qwen-2.5-72b-q4",
    "name": "Qwen 2.5 72B",
    "params_b": 72.7,
    "quantization": "Q4_K_M",
    "weight_gb": 47.42,
    "kv_per_1k_gb": 0.305,
    "max_context_k": 128,
    "mmlu_score": 86.1,
    "swe_bench_score": 28.4,
    "features": ["tool_use"],
    "notes": "80 layers, 8 KV heads, head_dim 128. GGUF Q4_K_M file size."
  },

  {
    "id": "qwen-2.5-vl-3b-q8",
    "name": "Qwen 2.5 VL 3B",
    "params_b": 3.09,
    "quantization": "Q8_0",
    "weight_gb": 3.29,
    "kv_per_1k_gb": 0.034,
    "max_context_k": 128,
    "mmlu_score": 65.6,
    "swe_bench_score": null,
    "features": ["vision"],
    "notes": "Vision-language model. Same text backbone as Qwen 2.5 3B. 36 layers, 2 KV heads, head_dim 128. GGUF Q8_0 file size."
  },
  {
    "id": "qwen-2.5-vl-3b-q4",
    "name": "Qwen 2.5 VL 3B",
    "params_b": 3.09,
    "quantization": "Q4_K_M",
    "weight_gb": 1.93,
    "kv_per_1k_gb": 0.034,
    "max_context_k": 128,
    "mmlu_score": 65.6,
    "swe_bench_score": null,
    "features": ["vision"],
    "notes": "Vision-language model. Same text backbone as Qwen 2.5 3B. 36 layers, 2 KV heads, head_dim 128. GGUF Q4_K_M file size."
  },

  {
    "id": "qwen-2.5-vl-7b-q8",
    "name": "Qwen 2.5 VL 7B",
    "params_b": 7.62,
    "quantization": "Q8_0",
    "weight_gb": 8.10,
    "kv_per_1k_gb": 0.053,
    "max_context_k": 128,
    "mmlu_score": 74.2,
    "swe_bench_score": null,
    "features": ["vision", "tool_use"],
    "notes": "Vision-language model. Same text backbone as Qwen 2.5 7B. 28 layers, 4 KV heads, head_dim 128. GGUF Q8_0 file size."
  },
  {
    "id": "qwen-2.5-vl-7b-q4",
    "name": "Qwen 2.5 VL 7B",
    "params_b": 7.62,
    "quantization": "Q4_K_M",
    "weight_gb": 4.68,
    "kv_per_1k_gb": 0.053,
    "max_context_k": 128,
    "mmlu_score": 74.2,
    "swe_bench_score": null,
    "features": ["vision", "tool_use"],
    "notes": "Vision-language model. Same text backbone as Qwen 2.5 7B. 28 layers, 4 KV heads, head_dim 128. GGUF Q4_K_M file size."
  },

  {
    "id": "qwen-2.5-vl-72b-q8",
    "name": "Qwen 2.5 VL 72B",
    "params_b": 72.7,
    "quantization": "Q8_0",
    "weight_gb": 77.30,
    "kv_per_1k_gb": 0.305,
    "max_context_k": 128,
    "mmlu_score": 86.1,
    "swe_bench_score": null,
    "features": ["vision", "tool_use"],
    "notes": "Vision-language model. Same text backbone as Qwen 2.5 72B. 80 layers, 8 KV heads, head_dim 128. GGUF Q8_0 file size."
  },
  {
    "id": "qwen-2.5-vl-72b-q4",
    "name": "Qwen 2.5 VL 72B",
    "params_b": 72.7,
    "quantization": "Q4_K_M",
    "weight_gb": 47.40,
    "kv_per_1k_gb": 0.305,
    "max_context_k": 128,
    "mmlu_score": 86.1,
    "swe_bench_score": null,
    "features": ["vision", "tool_use"],
    "notes": "Vision-language model. Same text backbone as Qwen 2.5 72B. 80 layers, 8 KV heads, head_dim 128. GGUF Q4_K_M file size."
  },

  {
    "id": "qwen3-8b-q8",
    "name": "Qwen3 8B",
    "params_b": 8.2,
    "quantization": "Q8_0",
    "weight_gb": 8.71,
    "kv_per_1k_gb": 0.137,
    "max_context_k": 128,
    "mmlu_score": 79.0,
    "swe_bench_score": null,
    "features": ["reasoning", "tool_use"],
    "notes": "Reasoning model (thinking/non-thinking modes). 36 layers, 8 KV heads, head_dim 128. GGUF Q8_0 file size."
  },
  {
    "id": "qwen3-8b-q4",
    "name": "Qwen3 8B",
    "params_b": 8.2,
    "quantization": "Q4_K_M",
    "weight_gb": 5.03,
    "kv_per_1k_gb": 0.137,
    "max_context_k": 128,
    "mmlu_score": 79.0,
    "swe_bench_score": null,
    "features": ["reasoning", "tool_use"],
    "notes": "Reasoning model (thinking/non-thinking modes). 36 layers, 8 KV heads, head_dim 128. GGUF Q4_K_M file size."
  },

  {
    "id": "qwen3-32b-q8",
    "name": "Qwen3 32B",
    "params_b": 32.8,
    "quantization": "Q8_0",
    "weight_gb": 34.8,
    "kv_per_1k_gb": 0.244,
    "max_context_k": 128,
    "mmlu_score": 85.0,
    "swe_bench_score": null,
    "features": ["reasoning", "tool_use"],
    "notes": "Reasoning model (thinking/non-thinking modes). 64 layers, 8 KV heads, head_dim 128. GGUF Q8_0 file size."
  },
  {
    "id": "qwen3-32b-q4",
    "name": "Qwen3 32B",
    "params_b": 32.8,
    "quantization": "Q4_K_M",
    "weight_gb": 19.8,
    "kv_per_1k_gb": 0.244,
    "max_context_k": 128,
    "mmlu_score": 85.0,
    "swe_bench_score": null,
    "features": ["reasoning", "tool_use"],
    "notes": "Reasoning model (thinking/non-thinking modes). 64 layers, 8 KV heads, head_dim 128. GGUF Q4_K_M file size."
  },

  {
    "id": "qwen3-next-80b-a3b-q8",
    "name": "Qwen3 Next 80B-A3B",
    "params_b": 80,
    "quantization": "Q8_0",
    "weight_gb": 84.8,
    "kv_per_1k_gb": 0.023,
    "max_context_k": 256,
    "mmlu_score": 90.0,
    "swe_bench_score": null,
    "features": ["tool_use"],
    "notes": "MoE: 80B total, 3B active (512 experts). Hybrid architecture (Gated DeltaNet + Gated Attention). Only 12 of 48 layers use attention (2 KV heads, head_dim 256). KV estimate covers attention layers only; DeltaNet layers use fixed-size recurrent state. Instruct-only (no thinking mode). GGUF Q8_0 file size."
  },
  {
    "id": "qwen3-next-80b-a3b-q4",
    "name": "Qwen3 Next 80B-A3B",
    "params_b": 80,
    "quantization": "Q4_K_M",
    "weight_gb": 48.4,
    "kv_per_1k_gb": 0.023,
    "max_context_k": 256,
    "mmlu_score": 90.0,
    "swe_bench_score": null,
    "features": ["tool_use"],
    "notes": "MoE: 80B total, 3B active (512 experts). Hybrid architecture (Gated DeltaNet + Gated Attention). Only 12 of 48 layers use attention (2 KV heads, head_dim 256). KV estimate covers attention layers only; DeltaNet layers use fixed-size recurrent state. Instruct-only (no thinking mode). GGUF Q4_K_M file size."
  },

  {
    "id": "deepseek-r1-distill-qwen-7b-q8",
    "name": "DeepSeek R1 Distill Qwen 7B",
    "params_b": 7.62,
    "quantization": "Q8_0",
    "weight_gb": 8.10,
    "kv_per_1k_gb": 0.053,
    "max_context_k": 128,
    "mmlu_score": 58.0,
    "swe_bench_score": 12.2,
    "features": ["reasoning"],
    "notes": "Reasoning model (distilled from R1). Based on Qwen 2.5 Math 7B. 28 layers, 4 KV heads, head_dim 128. GGUF Q8_0 file size."
  },
  {
    "id": "deepseek-r1-distill-qwen-7b-q4",
    "name": "DeepSeek R1 Distill Qwen 7B",
    "params_b": 7.62,
    "quantization": "Q4_K_M",
    "weight_gb": 4.68,
    "kv_per_1k_gb": 0.053,
    "max_context_k": 128,
    "mmlu_score": 58.0,
    "swe_bench_score": 12.2,
    "features": ["reasoning"],
    "notes": "Reasoning model (distilled from R1). Based on Qwen 2.5 Math 7B. 28 layers, 4 KV heads, head_dim 128. GGUF Q4_K_M file size."
  },

  {
    "id": "deepseek-r1-distill-qwen-14b-q8",
    "name": "DeepSeek R1 Distill Qwen 14B",
    "params_b": 14.7,
    "quantization": "Q8_0",
    "weight_gb": 15.70,
    "kv_per_1k_gb": 0.183,
    "max_context_k": 128,
    "mmlu_score": 72.0,
    "swe_bench_score": 20.4,
    "features": ["reasoning"],
    "notes": "Reasoning model (distilled from R1). Based on Qwen 2.5 14B. 48 layers, 8 KV heads, head_dim 128. GGUF Q8_0 file size."
  },
  {
    "id": "deepseek-r1-distill-qwen-14b-q4",
    "name": "DeepSeek R1 Distill Qwen 14B",
    "params_b": 14.7,
    "quantization": "Q4_K_M",
    "weight_gb": 8.99,
    "kv_per_1k_gb": 0.183,
    "max_context_k": 128,
    "mmlu_score": 72.0,
    "swe_bench_score": 20.4,
    "features": ["reasoning"],
    "notes": "Reasoning model (distilled from R1). Based on Qwen 2.5 14B. 48 layers, 8 KV heads, head_dim 128. GGUF Q4_K_M file size."
  },

  {
    "id": "deepseek-r1-distill-qwen-32b-q8",
    "name": "DeepSeek R1 Distill Qwen 32B",
    "params_b": 32.5,
    "quantization": "Q8_0",
    "weight_gb": 34.82,
    "kv_per_1k_gb": 0.244,
    "max_context_k": 128,
    "mmlu_score": 80.0,
    "swe_bench_score": 30.6,
    "features": ["reasoning"],
    "notes": "Reasoning model (distilled from R1). Based on Qwen 2.5 32B. 64 layers, 8 KV heads, head_dim 128. GGUF Q8_0 file size."
  },
  {
    "id": "deepseek-r1-distill-qwen-32b-q4",
    "name": "DeepSeek R1 Distill Qwen 32B",
    "params_b": 32.5,
    "quantization": "Q4_K_M",
    "weight_gb": 19.85,
    "kv_per_1k_gb": 0.244,
    "max_context_k": 128,
    "mmlu_score": 80.0,
    "swe_bench_score": 30.6,
    "features": ["reasoning"],
    "notes": "Reasoning model (distilled from R1). Based on Qwen 2.5 32B. 64 layers, 8 KV heads, head_dim 128. GGUF Q4_K_M file size."
  },

  {
    "id": "deepseek-r1-distill-llama-70b-q8",
    "name": "DeepSeek R1 Distill Llama 70B",
    "params_b": 70.6,
    "quantization": "Q8_0",
    "weight_gb": 74.98,
    "kv_per_1k_gb": 0.305,
    "max_context_k": 128,
    "mmlu_score": 84.0,
    "swe_bench_score": 27.2,
    "features": ["reasoning"],
    "notes": "Reasoning model (distilled from R1). Based on Llama 3.3 70B. 80 layers, 8 KV heads, head_dim 128. GGUF Q8_0 file size."
  },
  {
    "id": "deepseek-r1-distill-llama-70b-q4",
    "name": "DeepSeek R1 Distill Llama 70B",
    "params_b": 70.6,
    "quantization": "Q4_K_M",
    "weight_gb": 42.52,
    "kv_per_1k_gb": 0.305,
    "max_context_k": 128,
    "mmlu_score": 84.0,
    "swe_bench_score": 27.2,
    "features": ["reasoning"],
    "notes": "Reasoning model (distilled from R1). Based on Llama 3.3 70B. 80 layers, 8 KV heads, head_dim 128. GGUF Q4_K_M file size."
  },

  {
    "id": "qwq-32b-q8",
    "name": "QwQ 32B",
    "params_b": 32.5,
    "quantization": "Q8_0",
    "weight_gb": 34.90,
    "kv_per_1k_gb": 0.244,
    "max_context_k": 128,
    "mmlu_score": 83.0,
    "swe_bench_score": null,
    "features": ["reasoning", "tool_use"],
    "notes": "Reasoning model. Based on Qwen 2.5 32B. 64 layers, 8 KV heads, head_dim 128. GGUF Q8_0 file size."
  },
  {
    "id": "qwq-32b-q4",
    "name": "QwQ 32B",
    "params_b": 32.5,
    "quantization": "Q4_K_M",
    "weight_gb": 19.90,
    "kv_per_1k_gb": 0.244,
    "max_context_k": 128,
    "mmlu_score": 83.0,
    "swe_bench_score": null,
    "features": ["reasoning", "tool_use"],
    "notes": "Reasoning model. Based on Qwen 2.5 32B. 64 layers, 8 KV heads, head_dim 128. GGUF Q4_K_M file size."
  },

  {
    "id": "phi-4-mini-3.8b-q8",
    "name": "Phi-4 Mini 3.8B",
    "params_b": 3.84,
    "quantization": "Q8_0",
    "weight_gb": 4.08,
    "kv_per_1k_gb": 0.122,
    "max_context_k": 128,
    "mmlu_score": 70.0,
    "swe_bench_score": null,
    "features": [],
    "notes": "32 layers, 8 KV heads, head_dim 128. 200K vocabulary. GGUF Q8_0 file size."
  },
  {
    "id": "phi-4-mini-3.8b-q4",
    "name": "Phi-4 Mini 3.8B",
    "params_b": 3.84,
    "quantization": "Q4_K_M",
    "weight_gb": 2.49,
    "kv_per_1k_gb": 0.122,
    "max_context_k": 128,
    "mmlu_score": 70.0,
    "swe_bench_score": null,
    "features": [],
    "notes": "32 layers, 8 KV heads, head_dim 128. 200K vocabulary. GGUF Q4_K_M file size."
  },

  {
    "id": "phi-4-14b-q8",
    "name": "Phi-4 14B",
    "params_b": 14.7,
    "quantization": "Q8_0",
    "weight_gb": 15.58,
    "kv_per_1k_gb": 0.191,
    "max_context_k": 16,
    "mmlu_score": 84.8,
    "swe_bench_score": 18.6,
    "features": [],
    "notes": "40 layers, 10 KV heads, head_dim 128. GGUF Q8_0 file size."
  },
  {
    "id": "phi-4-14b-q4",
    "name": "Phi-4 14B",
    "params_b": 14.7,
    "quantization": "Q4_K_M",
    "weight_gb": 9.06,
    "kv_per_1k_gb": 0.191,
    "max_context_k": 16,
    "mmlu_score": 84.8,
    "swe_bench_score": 18.6,
    "features": [],
    "notes": "40 layers, 10 KV heads, head_dim 128. GGUF Q4_K_M file size."
  },

  {
    "id": "mistral-nemo-12b-q8",
    "name": "Mistral Nemo 12B",
    "params_b": 12.2,
    "quantization": "Q8_0",
    "weight_gb": 13.10,
    "kv_per_1k_gb": 0.191,
    "max_context_k": 128,
    "mmlu_score": 68.0,
    "swe_bench_score": null,
    "features": ["tool_use"],
    "notes": "Co-developed with NVIDIA. 40 layers, 8 KV heads, head_dim 160. GGUF Q8_0 file size."
  },
  {
    "id": "mistral-nemo-12b-q4",
    "name": "Mistral Nemo 12B",
    "params_b": 12.2,
    "quantization": "Q4_K_M",
    "weight_gb": 7.60,
    "kv_per_1k_gb": 0.191,
    "max_context_k": 128,
    "mmlu_score": 68.0,
    "swe_bench_score": null,
    "features": ["tool_use"],
    "notes": "Co-developed with NVIDIA. 40 layers, 8 KV heads, head_dim 160. GGUF Q4_K_M file size."
  },

  {
    "id": "mistral-small-3.1-24b-q8",
    "name": "Mistral Small 3.1 24B",
    "params_b": 23.6,
    "quantization": "Q8_0",
    "weight_gb": 25.05,
    "kv_per_1k_gb": 0.153,
    "max_context_k": 128,
    "mmlu_score": 81.0,
    "swe_bench_score": 20.8,
    "features": ["vision", "tool_use"],
    "notes": "40 layers, 8 KV heads, head_dim 128. GGUF Q8_0 file size."
  },
  {
    "id": "mistral-small-3.1-24b-q4",
    "name": "Mistral Small 3.1 24B",
    "params_b": 23.6,
    "quantization": "Q4_K_M",
    "weight_gb": 14.33,
    "kv_per_1k_gb": 0.153,
    "max_context_k": 128,
    "mmlu_score": 81.0,
    "swe_bench_score": 20.8,
    "features": ["vision", "tool_use"],
    "notes": "40 layers, 8 KV heads, head_dim 128. GGUF Q4_K_M file size."
  },

  {
    "id": "gemma-2-9b-q8",
    "name": "Gemma 2 9B",
    "params_b": 9.24,
    "quantization": "Q8_0",
    "weight_gb": 9.83,
    "kv_per_1k_gb": 0.321,
    "max_context_k": 8,
    "mmlu_score": 71.3,
    "swe_bench_score": null,
    "features": [],
    "notes": "42 layers, 8 KV heads, head_dim 256. GGUF Q8_0 file size."
  },
  {
    "id": "gemma-2-9b-q4",
    "name": "Gemma 2 9B",
    "params_b": 9.24,
    "quantization": "Q4_K_M",
    "weight_gb": 5.76,
    "kv_per_1k_gb": 0.321,
    "max_context_k": 8,
    "mmlu_score": 71.3,
    "swe_bench_score": null,
    "features": [],
    "notes": "42 layers, 8 KV heads, head_dim 256. GGUF Q4_K_M file size."
  },

  {
    "id": "gemma-2-27b-q8",
    "name": "Gemma 2 27B",
    "params_b": 27.2,
    "quantization": "Q8_0",
    "weight_gb": 28.94,
    "kv_per_1k_gb": 0.351,
    "max_context_k": 8,
    "mmlu_score": 75.2,
    "swe_bench_score": null,
    "features": [],
    "notes": "46 layers, 16 KV heads, head_dim 128. GGUF Q8_0 file size."
  },
  {
    "id": "gemma-2-27b-q4",
    "name": "Gemma 2 27B",
    "params_b": 27.2,
    "quantization": "Q4_K_M",
    "weight_gb": 16.65,
    "kv_per_1k_gb": 0.351,
    "max_context_k": 8,
    "mmlu_score": 75.2,
    "swe_bench_score": null,
    "features": [],
    "notes": "46 layers, 16 KV heads, head_dim 128. GGUF Q4_K_M file size."
  },

  {
    "id": "gemma-3-4b-q8",
    "name": "Gemma 3 4B",
    "params_b": 3.88,
    "quantization": "Q8_0",
    "weight_gb": 4.13,
    "kv_per_1k_gb": 0.147,
    "max_context_k": 128,
    "mmlu_score": 63.0,
    "swe_bench_score": null,
    "features": ["vision"],
    "notes": "34 layers (5:1 local:global, sw=1024), head_dim 256. KV estimate from Gemma 3 tech report. GGUF Q8_0 file size."
  },
  {
    "id": "gemma-3-4b-q4",
    "name": "Gemma 3 4B",
    "params_b": 3.88,
    "quantization": "Q4_K_M",
    "weight_gb": 2.49,
    "kv_per_1k_gb": 0.147,
    "max_context_k": 128,
    "mmlu_score": 63.0,
    "swe_bench_score": null,
    "features": ["vision"],
    "notes": "34 layers (5:1 local:global, sw=1024), head_dim 256. KV estimate from Gemma 3 tech report. GGUF Q4_K_M file size."
  },

  {
    "id": "gemma-3-12b-q8",
    "name": "Gemma 3 12B",
    "params_b": 11.8,
    "quantization": "Q8_0",
    "weight_gb": 12.51,
    "kv_per_1k_gb": 0.466,
    "max_context_k": 128,
    "mmlu_score": 74.0,
    "swe_bench_score": 12.8,
    "features": ["vision"],
    "notes": "48 layers (5:1 local:global, sw=1024), head_dim 256. KV estimate from Gemma 3 tech report. GGUF Q8_0 file size."
  },
  {
    "id": "gemma-3-12b-q4",
    "name": "Gemma 3 12B",
    "params_b": 11.8,
    "quantization": "Q4_K_M",
    "weight_gb": 7.30,
    "kv_per_1k_gb": 0.466,
    "max_context_k": 128,
    "mmlu_score": 74.0,
    "swe_bench_score": 12.8,
    "features": ["vision"],
    "notes": "48 layers (5:1 local:global, sw=1024), head_dim 256. KV estimate from Gemma 3 tech report. GGUF Q4_K_M file size."
  },

  {
    "id": "gemma-3-27b-q8",
    "name": "Gemma 3 27B",
    "params_b": 27.0,
    "quantization": "Q8_0",
    "weight_gb": 28.71,
    "kv_per_1k_gb": 0.584,
    "max_context_k": 128,
    "mmlu_score": 78.0,
    "swe_bench_score": 17.4,
    "features": ["vision"],
    "notes": "62 layers (5:1 local:global, sw=1024), head_dim 256. KV estimate from Gemma 3 tech report. GGUF Q8_0 file size."
  },
  {
    "id": "gemma-3-27b-q4",
    "name": "Gemma 3 27B",
    "params_b": 27.0,
    "quantization": "Q4_K_M",
    "weight_gb": 16.55,
    "kv_per_1k_gb": 0.584,
    "max_context_k": 128,
    "mmlu_score": 78.0,
    "swe_bench_score": 17.4,
    "features": ["vision"],
    "notes": "62 layers (5:1 local:global, sw=1024), head_dim 256. KV estimate from Gemma 3 tech report. GGUF Q4_K_M file size."
  },

  {
    "id": "glm-4.7-flash-q8",
    "name": "GLM-4.7 Flash",
    "params_b": 31,
    "quantization": "Q8_0",
    "weight_gb": 31.8,
    "kv_per_1k_gb": 0.050,
    "max_context_k": 128,
    "mmlu_score": 82.0,
    "swe_bench_score": null,
    "features": ["reasoning", "tool_use"],
    "notes": "MoE: 30B total, 3B active (64 experts, 4 active). 47 layers, uses MLA (kv_lora_rank=512). KV estimate is for compressed MLA cache. GGUF Q8_0 file size."
  },
  {
    "id": "glm-4.7-flash-q4",
    "name": "GLM-4.7 Flash",
    "params_b": 31,
    "quantization": "Q4_K_M",
    "weight_gb": 18.3,
    "kv_per_1k_gb": 0.050,
    "max_context_k": 128,
    "mmlu_score": 82.0,
    "swe_bench_score": null,
    "features": ["reasoning", "tool_use"],
    "notes": "MoE: 30B total, 3B active (64 experts, 4 active). 47 layers, uses MLA (kv_lora_rank=512). KV estimate is for compressed MLA cache. GGUF Q4_K_M file size."
  },

  {
    "id": "nemotron-nano-12b-q8",
    "name": "Nemotron Nano 12B",
    "params_b": 12.6,
    "quantization": "Q8_0",
    "weight_gb": 13.09,
    "kv_per_1k_gb": 0.023,
    "max_context_k": 128,
    "mmlu_score": 76.0,
    "swe_bench_score": null,
    "features": ["reasoning", "tool_use"],
    "notes": "Hybrid Mamba-Transformer (6 attention + 56 Mamba-2 layers). 8 KV heads, head_dim 128 on attention layers only. KV estimate covers attention layers; Mamba layers use fixed-size state (~140 MB). GGUF Q8_0 file size."
  },
  {
    "id": "nemotron-nano-12b-q4",
    "name": "Nemotron Nano 12B",
    "params_b": 12.6,
    "quantization": "Q4_K_M",
    "weight_gb": 7.37,
    "kv_per_1k_gb": 0.023,
    "max_context_k": 128,
    "mmlu_score": 76.0,
    "swe_bench_score": null,
    "features": ["reasoning", "tool_use"],
    "notes": "Hybrid Mamba-Transformer (6 attention + 56 Mamba-2 layers). 8 KV heads, head_dim 128 on attention layers only. KV estimate covers attention layers; Mamba layers use fixed-size state (~140 MB). GGUF Q4_K_M file size."
  },

  {
    "id": "qwen-2.5-coder-7b-q8",
    "name": "Qwen 2.5 Coder 7B",
    "params_b": 7.62,
    "quantization": "Q8_0",
    "weight_gb": 8.10,
    "kv_per_1k_gb": 0.053,
    "max_context_k": 128,
    "mmlu_score": 68.0,
    "swe_bench_score": 14.8,
    "features": [],
    "notes": "Code model. Based on Qwen 2.5 7B. 28 layers, 4 KV heads, head_dim 128. GGUF Q8_0 file size."
  },
  {
    "id": "qwen-2.5-coder-7b-q4",
    "name": "Qwen 2.5 Coder 7B",
    "params_b": 7.62,
    "quantization": "Q4_K_M",
    "weight_gb": 4.68,
    "kv_per_1k_gb": 0.053,
    "max_context_k": 128,
    "mmlu_score": 68.0,
    "swe_bench_score": 14.8,
    "features": [],
    "notes": "Code model. Based on Qwen 2.5 7B. 28 layers, 4 KV heads, head_dim 128. GGUF Q4_K_M file size."
  },

  {
    "id": "qwen-2.5-coder-14b-q8",
    "name": "Qwen 2.5 Coder 14B",
    "params_b": 14.7,
    "quantization": "Q8_0",
    "weight_gb": 15.70,
    "kv_per_1k_gb": 0.183,
    "max_context_k": 128,
    "mmlu_score": 74.0,
    "swe_bench_score": 22.6,
    "features": [],
    "notes": "Code model. Based on Qwen 2.5 14B. 48 layers, 8 KV heads, head_dim 128. GGUF Q8_0 file size."
  },
  {
    "id": "qwen-2.5-coder-14b-q4",
    "name": "Qwen 2.5 Coder 14B",
    "params_b": 14.7,
    "quantization": "Q4_K_M",
    "weight_gb": 8.99,
    "kv_per_1k_gb": 0.183,
    "max_context_k": 128,
    "mmlu_score": 74.0,
    "swe_bench_score": 22.6,
    "features": [],
    "notes": "Code model. Based on Qwen 2.5 14B. 48 layers, 8 KV heads, head_dim 128. GGUF Q4_K_M file size."
  },

  {
    "id": "qwen-2.5-coder-32b-q8",
    "name": "Qwen 2.5 Coder 32B",
    "params_b": 32.5,
    "quantization": "Q8_0",
    "weight_gb": 34.82,
    "kv_per_1k_gb": 0.244,
    "max_context_k": 128,
    "mmlu_score": 79.0,
    "swe_bench_score": 33.4,
    "features": [],
    "notes": "Code model. Based on Qwen 2.5 32B. 64 layers, 8 KV heads, head_dim 128. GGUF Q8_0 file size."
  },
  {
    "id": "qwen-2.5-coder-32b-q4",
    "name": "Qwen 2.5 Coder 32B",
    "params_b": 32.5,
    "quantization": "Q4_K_M",
    "weight_gb": 19.85,
    "kv_per_1k_gb": 0.244,
    "max_context_k": 128,
    "mmlu_score": 79.0,
    "swe_bench_score": 33.4,
    "features": [],
    "notes": "Code model. Based on Qwen 2.5 32B. 64 layers, 8 KV heads, head_dim 128. GGUF Q4_K_M file size."
  },

  {
    "id": "codestral-22b-q8",
    "name": "Codestral 22B",
    "params_b": 22.2,
    "quantization": "Q8_0",
    "weight_gb": 23.64,
    "kv_per_1k_gb": 0.214,
    "max_context_k": 32,
    "mmlu_score": 62.0,
    "swe_bench_score": null,
    "features": [],
    "notes": "Code model. Mistral architecture. 56 layers, 8 KV heads, head_dim 128. GGUF Q8_0 file size."
  },
  {
    "id": "codestral-22b-q4",
    "name": "Codestral 22B",
    "params_b": 22.2,
    "quantization": "Q4_K_M",
    "weight_gb": 13.30,
    "kv_per_1k_gb": 0.214,
    "max_context_k": 32,
    "mmlu_score": 62.0,
    "swe_bench_score": null,
    "features": [],
    "notes": "Code model. Mistral architecture. 56 layers, 8 KV heads, head_dim 128. GGUF Q4_K_M file size."
  },

  {
    "id": "starcoder2-15b-q8",
    "name": "StarCoder2 15B",
    "params_b": 15.6,
    "quantization": "Q8_0",
    "weight_gb": 17.00,
    "kv_per_1k_gb": 0.076,
    "max_context_k": 16,
    "mmlu_score": 51.4,
    "swe_bench_score": null,
    "features": [],
    "notes": "Code model. BigCode. 40 layers, 4 KV heads, head_dim 128. Trained on 600+ languages. GGUF Q8_0 file size."
  },
  {
    "id": "starcoder2-15b-q4",
    "name": "StarCoder2 15B",
    "params_b": 15.6,
    "quantization": "Q4_K_M",
    "weight_gb": 9.86,
    "kv_per_1k_gb": 0.076,
    "max_context_k": 16,
    "mmlu_score": 51.4,
    "swe_bench_score": null,
    "features": [],
    "notes": "Code model. BigCode. 40 layers, 4 KV heads, head_dim 128. Trained on 600+ languages. GGUF Q4_K_M file size."
  }
]

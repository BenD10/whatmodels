[
  {
    "id": "llama-3.2-1b-q8",
    "name": "Llama 3.2 1B",
    "params_b": 1.24,
    "quantization": "Q8_0",
    "weight_gb": 1.32,
    "kv_per_1k_gb": 0.031,
    "max_context_k": 128,
    "mmlu_score": 49.3,
    "notes": "16 layers, 8 KV heads, head_dim 64. Weights from bartowski GGUF Q8_0."
  },
  {
    "id": "llama-3.2-1b-q4",
    "name": "Llama 3.2 1B",
    "params_b": 1.24,
    "quantization": "Q4_K_M",
    "weight_gb": 0.81,
    "kv_per_1k_gb": 0.031,
    "max_context_k": 128,
    "mmlu_score": 49.3,
    "notes": "16 layers, 8 KV heads, head_dim 64. Weights from bartowski GGUF Q4_K_M."
  },

  {
    "id": "llama-3.2-3b-fp16",
    "name": "Llama 3.2 3B",
    "params_b": 3.21,
    "quantization": "fp16",
    "weight_gb": 6.43,
    "kv_per_1k_gb": 0.107,
    "max_context_k": 128,
    "mmlu_score": 63.4,
    "notes": "28 layers, 8 KV heads, head_dim 128. Weights from bartowski GGUF f16."
  },
  {
    "id": "llama-3.2-3b-q8",
    "name": "Llama 3.2 3B",
    "params_b": 3.21,
    "quantization": "Q8_0",
    "weight_gb": 3.42,
    "kv_per_1k_gb": 0.107,
    "max_context_k": 128,
    "mmlu_score": 63.4,
    "notes": "28 layers, 8 KV heads, head_dim 128. Weights from bartowski GGUF Q8_0."
  },
  {
    "id": "llama-3.2-3b-q4",
    "name": "Llama 3.2 3B",
    "params_b": 3.21,
    "quantization": "Q4_K_M",
    "weight_gb": 2.02,
    "kv_per_1k_gb": 0.107,
    "max_context_k": 128,
    "mmlu_score": 63.4,
    "notes": "28 layers, 8 KV heads, head_dim 128. Weights from bartowski GGUF Q4_K_M."
  },

  {
    "id": "llama-3.1-8b-fp16",
    "name": "Llama 3.1 8B",
    "params_b": 8.03,
    "quantization": "fp16",
    "weight_gb": 16.1,
    "kv_per_1k_gb": 0.122,
    "max_context_k": 128,
    "mmlu_score": 69.4,
    "notes": "32 layers, 8 KV heads, head_dim 128. Estimated from f32 (32.1 GB) / 2."
  },
  {
    "id": "llama-3.1-8b-q8",
    "name": "Llama 3.1 8B",
    "params_b": 8.03,
    "quantization": "Q8_0",
    "weight_gb": 8.54,
    "kv_per_1k_gb": 0.122,
    "max_context_k": 128,
    "mmlu_score": 69.4,
    "notes": "32 layers, 8 KV heads, head_dim 128. Weights from bartowski GGUF Q8_0."
  },
  {
    "id": "llama-3.1-8b-q4",
    "name": "Llama 3.1 8B",
    "params_b": 8.03,
    "quantization": "Q4_K_M",
    "weight_gb": 4.92,
    "kv_per_1k_gb": 0.122,
    "max_context_k": 128,
    "mmlu_score": 69.4,
    "notes": "32 layers, 8 KV heads, head_dim 128. Weights from bartowski GGUF Q4_K_M."
  },

  {
    "id": "llama-3.3-70b-q8",
    "name": "Llama 3.3 70B",
    "params_b": 70.6,
    "quantization": "Q8_0",
    "weight_gb": 74.98,
    "kv_per_1k_gb": 0.305,
    "max_context_k": 128,
    "mmlu_score": 86.0,
    "notes": "80 layers, 8 KV heads, head_dim 128. Weights from bartowski GGUF Q8_0."
  },
  {
    "id": "llama-3.3-70b-q4",
    "name": "Llama 3.3 70B",
    "params_b": 70.6,
    "quantization": "Q4_K_M",
    "weight_gb": 42.52,
    "kv_per_1k_gb": 0.305,
    "max_context_k": 128,
    "mmlu_score": 86.0,
    "notes": "80 layers, 8 KV heads, head_dim 128. Weights from bartowski GGUF Q4_K_M."
  },

  {
    "id": "qwen-2.5-3b-q8",
    "name": "Qwen 2.5 3B",
    "params_b": 3.09,
    "quantization": "Q8_0",
    "weight_gb": 3.29,
    "kv_per_1k_gb": 0.034,
    "max_context_k": 32,
    "mmlu_score": 65.6,
    "notes": "36 layers, 2 KV heads, head_dim 128. Weights from bartowski GGUF Q8_0."
  },
  {
    "id": "qwen-2.5-3b-q4",
    "name": "Qwen 2.5 3B",
    "params_b": 3.09,
    "quantization": "Q4_K_M",
    "weight_gb": 1.93,
    "kv_per_1k_gb": 0.034,
    "max_context_k": 32,
    "mmlu_score": 65.6,
    "notes": "36 layers, 2 KV heads, head_dim 128. Weights from bartowski GGUF Q4_K_M."
  },

  {
    "id": "qwen-2.5-7b-q8",
    "name": "Qwen 2.5 7B",
    "params_b": 7.62,
    "quantization": "Q8_0",
    "weight_gb": 8.10,
    "kv_per_1k_gb": 0.053,
    "max_context_k": 128,
    "mmlu_score": 74.2,
    "notes": "28 layers, 4 KV heads, head_dim 128. Weights from bartowski GGUF Q8_0."
  },
  {
    "id": "qwen-2.5-7b-q4",
    "name": "Qwen 2.5 7B",
    "params_b": 7.62,
    "quantization": "Q4_K_M",
    "weight_gb": 4.68,
    "kv_per_1k_gb": 0.053,
    "max_context_k": 128,
    "mmlu_score": 74.2,
    "notes": "28 layers, 4 KV heads, head_dim 128. Weights from bartowski GGUF Q4_K_M."
  },

  {
    "id": "qwen-2.5-14b-q8",
    "name": "Qwen 2.5 14B",
    "params_b": 14.7,
    "quantization": "Q8_0",
    "weight_gb": 15.70,
    "kv_per_1k_gb": 0.183,
    "max_context_k": 128,
    "mmlu_score": 79.9,
    "notes": "48 layers, 8 KV heads, head_dim 128. Weights from bartowski GGUF Q8_0."
  },
  {
    "id": "qwen-2.5-14b-q4",
    "name": "Qwen 2.5 14B",
    "params_b": 14.7,
    "quantization": "Q4_K_M",
    "weight_gb": 8.99,
    "kv_per_1k_gb": 0.183,
    "max_context_k": 128,
    "mmlu_score": 79.9,
    "notes": "48 layers, 8 KV heads, head_dim 128. Weights from bartowski GGUF Q4_K_M."
  },

  {
    "id": "qwen-2.5-32b-q8",
    "name": "Qwen 2.5 32B",
    "params_b": 32.5,
    "quantization": "Q8_0",
    "weight_gb": 34.82,
    "kv_per_1k_gb": 0.244,
    "max_context_k": 128,
    "mmlu_score": 83.3,
    "notes": "64 layers, 8 KV heads, head_dim 128. Weights from bartowski GGUF Q8_0."
  },
  {
    "id": "qwen-2.5-32b-q4",
    "name": "Qwen 2.5 32B",
    "params_b": 32.5,
    "quantization": "Q4_K_M",
    "weight_gb": 19.85,
    "kv_per_1k_gb": 0.244,
    "max_context_k": 128,
    "mmlu_score": 83.3,
    "notes": "64 layers, 8 KV heads, head_dim 128. Weights from bartowski GGUF Q4_K_M."
  },

  {
    "id": "qwen-2.5-72b-q8",
    "name": "Qwen 2.5 72B",
    "params_b": 72.7,
    "quantization": "Q8_0",
    "weight_gb": 77.26,
    "kv_per_1k_gb": 0.305,
    "max_context_k": 128,
    "mmlu_score": 86.1,
    "notes": "80 layers, 8 KV heads, head_dim 128. Weights from bartowski GGUF Q8_0."
  },
  {
    "id": "qwen-2.5-72b-q4",
    "name": "Qwen 2.5 72B",
    "params_b": 72.7,
    "quantization": "Q4_K_M",
    "weight_gb": 47.42,
    "kv_per_1k_gb": 0.305,
    "max_context_k": 128,
    "mmlu_score": 86.1,
    "notes": "80 layers, 8 KV heads, head_dim 128. Weights from bartowski GGUF Q4_K_M."
  },

  {
    "id": "deepseek-r1-distill-qwen-7b-q8",
    "name": "DeepSeek R1 Distill Qwen 7B",
    "params_b": 7.62,
    "quantization": "Q8_0",
    "weight_gb": 8.10,
    "kv_per_1k_gb": 0.053,
    "max_context_k": 128,
    "mmlu_score": 58.0,
    "notes": "Reasoning model (distilled from R1). Based on Qwen 2.5 Math 7B. 28 layers, 4 KV heads, head_dim 128. Weights from bartowski GGUF Q8_0."
  },
  {
    "id": "deepseek-r1-distill-qwen-7b-q4",
    "name": "DeepSeek R1 Distill Qwen 7B",
    "params_b": 7.62,
    "quantization": "Q4_K_M",
    "weight_gb": 4.68,
    "kv_per_1k_gb": 0.053,
    "max_context_k": 128,
    "mmlu_score": 58.0,
    "notes": "Reasoning model (distilled from R1). Based on Qwen 2.5 Math 7B. 28 layers, 4 KV heads, head_dim 128. Weights from bartowski GGUF Q4_K_M."
  },

  {
    "id": "deepseek-r1-distill-qwen-14b-q8",
    "name": "DeepSeek R1 Distill Qwen 14B",
    "params_b": 14.7,
    "quantization": "Q8_0",
    "weight_gb": 15.70,
    "kv_per_1k_gb": 0.183,
    "max_context_k": 128,
    "mmlu_score": 72.0,
    "notes": "Reasoning model (distilled from R1). Based on Qwen 2.5 14B. 48 layers, 8 KV heads, head_dim 128. Weights from bartowski GGUF Q8_0."
  },
  {
    "id": "deepseek-r1-distill-qwen-14b-q4",
    "name": "DeepSeek R1 Distill Qwen 14B",
    "params_b": 14.7,
    "quantization": "Q4_K_M",
    "weight_gb": 9.13,
    "kv_per_1k_gb": 0.183,
    "max_context_k": 128,
    "mmlu_score": 72.0,
    "notes": "Reasoning model (distilled from R1). Based on Qwen 2.5 14B. 48 layers, 8 KV heads, head_dim 128. Weights from bartowski GGUF Q4_K_M."
  },

  {
    "id": "deepseek-r1-distill-qwen-32b-q8",
    "name": "DeepSeek R1 Distill Qwen 32B",
    "params_b": 32.5,
    "quantization": "Q8_0",
    "weight_gb": 34.82,
    "kv_per_1k_gb": 0.244,
    "max_context_k": 128,
    "mmlu_score": 80.0,
    "notes": "Reasoning model (distilled from R1). Based on Qwen 2.5 32B. 64 layers, 8 KV heads, head_dim 128. Weights from bartowski GGUF Q8_0."
  },
  {
    "id": "deepseek-r1-distill-qwen-32b-q4",
    "name": "DeepSeek R1 Distill Qwen 32B",
    "params_b": 32.5,
    "quantization": "Q4_K_M",
    "weight_gb": 19.90,
    "kv_per_1k_gb": 0.244,
    "max_context_k": 128,
    "mmlu_score": 80.0,
    "notes": "Reasoning model (distilled from R1). Based on Qwen 2.5 32B. 64 layers, 8 KV heads, head_dim 128. Weights from bartowski GGUF Q4_K_M."
  },

  {
    "id": "deepseek-r1-distill-llama-70b-q8",
    "name": "DeepSeek R1 Distill Llama 70B",
    "params_b": 70.6,
    "quantization": "Q8_0",
    "weight_gb": 74.98,
    "kv_per_1k_gb": 0.305,
    "max_context_k": 128,
    "mmlu_score": 84.0,
    "notes": "Reasoning model (distilled from R1). Based on Llama 3.3 70B. 80 layers, 8 KV heads, head_dim 128. Weights from bartowski GGUF Q8_0."
  },
  {
    "id": "deepseek-r1-distill-llama-70b-q4",
    "name": "DeepSeek R1 Distill Llama 70B",
    "params_b": 70.6,
    "quantization": "Q4_K_M",
    "weight_gb": 42.52,
    "kv_per_1k_gb": 0.305,
    "max_context_k": 128,
    "mmlu_score": 84.0,
    "notes": "Reasoning model (distilled from R1). Based on Llama 3.3 70B. 80 layers, 8 KV heads, head_dim 128. Weights from bartowski GGUF Q4_K_M."
  },

  {
    "id": "phi-4-14b-q8",
    "name": "Phi-4 14B",
    "params_b": 14.7,
    "quantization": "Q8_0",
    "weight_gb": 15.58,
    "kv_per_1k_gb": 0.191,
    "max_context_k": 16,
    "mmlu_score": 84.8,
    "notes": "40 layers, 10 KV heads, head_dim 128. Weights from bartowski GGUF Q8_0."
  },
  {
    "id": "phi-4-14b-q4",
    "name": "Phi-4 14B",
    "params_b": 14.7,
    "quantization": "Q4_K_M",
    "weight_gb": 9.06,
    "kv_per_1k_gb": 0.191,
    "max_context_k": 16,
    "mmlu_score": 84.8,
    "notes": "40 layers, 10 KV heads, head_dim 128. Weights from bartowski GGUF Q4_K_M."
  },

  {
    "id": "mistral-small-3.1-24b-q8",
    "name": "Mistral Small 3.1 24B",
    "params_b": 23.6,
    "quantization": "Q8_0",
    "weight_gb": 25.05,
    "kv_per_1k_gb": 0.153,
    "max_context_k": 128,
    "mmlu_score": 81.0,
    "notes": "40 layers, 8 KV heads, head_dim 128. Weights from bartowski GGUF Q8_0."
  },
  {
    "id": "mistral-small-3.1-24b-q4",
    "name": "Mistral Small 3.1 24B",
    "params_b": 23.6,
    "quantization": "Q4_K_M",
    "weight_gb": 14.33,
    "kv_per_1k_gb": 0.153,
    "max_context_k": 128,
    "mmlu_score": 81.0,
    "notes": "40 layers, 8 KV heads, head_dim 128. Weights from bartowski GGUF Q4_K_M."
  },

  {
    "id": "gemma-2-9b-q8",
    "name": "Gemma 2 9B",
    "params_b": 9.24,
    "quantization": "Q8_0",
    "weight_gb": 9.83,
    "kv_per_1k_gb": 0.321,
    "max_context_k": 8,
    "mmlu_score": 71.3,
    "notes": "42 layers, 8 KV heads, head_dim 256. Weights from bartowski GGUF Q8_0."
  },
  {
    "id": "gemma-2-9b-q4",
    "name": "Gemma 2 9B",
    "params_b": 9.24,
    "quantization": "Q4_K_M",
    "weight_gb": 5.76,
    "kv_per_1k_gb": 0.321,
    "max_context_k": 8,
    "mmlu_score": 71.3,
    "notes": "42 layers, 8 KV heads, head_dim 256. Weights from bartowski GGUF Q4_K_M."
  },

  {
    "id": "gemma-2-27b-q8",
    "name": "Gemma 2 27B",
    "params_b": 27.2,
    "quantization": "Q8_0",
    "weight_gb": 28.94,
    "kv_per_1k_gb": 0.351,
    "max_context_k": 8,
    "mmlu_score": 75.2,
    "notes": "46 layers, 16 KV heads, head_dim 128. Weights from bartowski GGUF Q8_0."
  },
  {
    "id": "gemma-2-27b-q4",
    "name": "Gemma 2 27B",
    "params_b": 27.2,
    "quantization": "Q4_K_M",
    "weight_gb": 16.65,
    "kv_per_1k_gb": 0.351,
    "max_context_k": 8,
    "mmlu_score": 75.2,
    "notes": "46 layers, 16 KV heads, head_dim 128. Weights from bartowski GGUF Q4_K_M."
  },

  {
    "id": "gemma-3-4b-q8",
    "name": "Gemma 3 4B",
    "params_b": 3.88,
    "quantization": "Q8_0",
    "weight_gb": 4.13,
    "kv_per_1k_gb": 0.147,
    "max_context_k": 128,
    "mmlu_score": 63.0,
    "notes": "34 layers (5:1 local:global, sw=1024), head_dim 256. KV estimate from Gemma 3 tech report. Weights from bartowski GGUF Q8_0."
  },
  {
    "id": "gemma-3-4b-q4",
    "name": "Gemma 3 4B",
    "params_b": 3.88,
    "quantization": "Q4_K_M",
    "weight_gb": 2.49,
    "kv_per_1k_gb": 0.147,
    "max_context_k": 128,
    "mmlu_score": 63.0,
    "notes": "34 layers (5:1 local:global, sw=1024), head_dim 256. KV estimate from Gemma 3 tech report. Weights from bartowski GGUF Q4_K_M."
  },

  {
    "id": "gemma-3-12b-q8",
    "name": "Gemma 3 12B",
    "params_b": 11.8,
    "quantization": "Q8_0",
    "weight_gb": 12.51,
    "kv_per_1k_gb": 0.466,
    "max_context_k": 128,
    "mmlu_score": 74.0,
    "notes": "48 layers (5:1 local:global, sw=1024), head_dim 256. KV estimate from Gemma 3 tech report. Weights from bartowski GGUF Q8_0."
  },
  {
    "id": "gemma-3-12b-q4",
    "name": "Gemma 3 12B",
    "params_b": 11.8,
    "quantization": "Q4_K_M",
    "weight_gb": 7.30,
    "kv_per_1k_gb": 0.466,
    "max_context_k": 128,
    "mmlu_score": 74.0,
    "notes": "48 layers (5:1 local:global, sw=1024), head_dim 256. KV estimate from Gemma 3 tech report. Weights from bartowski GGUF Q4_K_M."
  },

  {
    "id": "gemma-3-27b-q8",
    "name": "Gemma 3 27B",
    "params_b": 27.0,
    "quantization": "Q8_0",
    "weight_gb": 28.71,
    "kv_per_1k_gb": 0.584,
    "max_context_k": 128,
    "mmlu_score": 78.0,
    "notes": "62 layers (5:1 local:global, sw=1024), head_dim 256. KV estimate from Gemma 3 tech report. Weights from bartowski GGUF Q8_0."
  },
  {
    "id": "gemma-3-27b-q4",
    "name": "Gemma 3 27B",
    "params_b": 27.0,
    "quantization": "Q4_K_M",
    "weight_gb": 16.55,
    "kv_per_1k_gb": 0.584,
    "max_context_k": 128,
    "mmlu_score": 78.0,
    "notes": "62 layers (5:1 local:global, sw=1024), head_dim 256. KV estimate from Gemma 3 tech report. Weights from bartowski GGUF Q4_K_M."
  },

  {
    "id": "qwen-2.5-coder-7b-q8",
    "name": "Qwen 2.5 Coder 7B",
    "params_b": 7.62,
    "quantization": "Q8_0",
    "weight_gb": 8.10,
    "kv_per_1k_gb": 0.053,
    "max_context_k": 128,
    "mmlu_score": 68.0,
    "notes": "Code model. Based on Qwen 2.5 7B. 28 layers, 4 KV heads, head_dim 128. Weights from bartowski GGUF Q8_0."
  },
  {
    "id": "qwen-2.5-coder-7b-q4",
    "name": "Qwen 2.5 Coder 7B",
    "params_b": 7.62,
    "quantization": "Q4_K_M",
    "weight_gb": 4.68,
    "kv_per_1k_gb": 0.053,
    "max_context_k": 128,
    "mmlu_score": 68.0,
    "notes": "Code model. Based on Qwen 2.5 7B. 28 layers, 4 KV heads, head_dim 128. Weights from bartowski GGUF Q4_K_M."
  },

  {
    "id": "qwen-2.5-coder-14b-q8",
    "name": "Qwen 2.5 Coder 14B",
    "params_b": 14.7,
    "quantization": "Q8_0",
    "weight_gb": 15.70,
    "kv_per_1k_gb": 0.183,
    "max_context_k": 128,
    "mmlu_score": 74.0,
    "notes": "Code model. Based on Qwen 2.5 14B. 48 layers, 8 KV heads, head_dim 128. Weights from bartowski GGUF Q8_0."
  },
  {
    "id": "qwen-2.5-coder-14b-q4",
    "name": "Qwen 2.5 Coder 14B",
    "params_b": 14.7,
    "quantization": "Q4_K_M",
    "weight_gb": 8.99,
    "kv_per_1k_gb": 0.183,
    "max_context_k": 128,
    "mmlu_score": 74.0,
    "notes": "Code model. Based on Qwen 2.5 14B. 48 layers, 8 KV heads, head_dim 128. Weights from bartowski GGUF Q4_K_M."
  },

  {
    "id": "qwen-2.5-coder-32b-q8",
    "name": "Qwen 2.5 Coder 32B",
    "params_b": 32.5,
    "quantization": "Q8_0",
    "weight_gb": 34.82,
    "kv_per_1k_gb": 0.244,
    "max_context_k": 128,
    "mmlu_score": 79.0,
    "notes": "Code model. Based on Qwen 2.5 32B. 64 layers, 8 KV heads, head_dim 128. Weights from bartowski GGUF Q8_0."
  },
  {
    "id": "qwen-2.5-coder-32b-q4",
    "name": "Qwen 2.5 Coder 32B",
    "params_b": 32.5,
    "quantization": "Q4_K_M",
    "weight_gb": 19.85,
    "kv_per_1k_gb": 0.244,
    "max_context_k": 128,
    "mmlu_score": 79.0,
    "notes": "Code model. Based on Qwen 2.5 32B. 64 layers, 8 KV heads, head_dim 128. Weights from bartowski GGUF Q4_K_M."
  }
]
